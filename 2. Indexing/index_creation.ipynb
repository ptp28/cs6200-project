{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c919c43e-24e6-4ec5-b281-ad7a12565b30",
   "metadata": {},
   "source": [
    "## Merging inlinks and outlinks data from multiple threads\n",
    "\n",
    "### Pre-Requisites\n",
    " - Have a folder called 'inlinks' and 'outlinks' which has 1 json file from each thread of the crawler\n",
    " - Create a folder called 'final_index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86e24e0e-c98f-4b9a-9f3c-688a85b5eba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecfe39d4-8679-465b-aff6-0eeb67265c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_OUTPUT_FOLDER='./index_op'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4ce1762-a298-40c4-b696-b13d170de477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder paths\n",
    "INLINKS_FOLDER='./inlinks'\n",
    "OUTLINKS_FOLDER='./outlinks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba5ed3ff-fc11-4186-ac54-12620fbb1b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patterns\n",
    "DOCNO_PATTERN = r\"<DOCNO>(.*?)<\\\\DOCNO>\"\n",
    "TEXT_PATTERN = r\"<TEXT>(.*?)<\\\\TEXT>\"\n",
    "HEAD_PATTERN = r\"<HEAD>(.*?)<\\\\HEAD>\"\n",
    "DOC_PATTERN = r\"<DOC>(.*?)<\\\\DOC>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b688fa34-914b-4617-970e-fb3efc1db88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging process complete for inlinks\n"
     ]
    }
   ],
   "source": [
    "inlinks_set={}\n",
    "for inlink_file in os.listdir(INLINKS_FOLDER):\n",
    "    curr_path=os.path.join(os.getcwd(),f'{INLINKS_FOLDER}/{inlink_file}')\n",
    "    with open(curr_path,'r') as curr_file:\n",
    "        inlink_data=json.load(curr_file)\n",
    "        for url,inlink_list in inlink_data.items():\n",
    "            if url in inlinks_set:\n",
    "                for link in inlink_list:\n",
    "                    if inlinks_set[url].count(link) == 0:\n",
    "                        inlinks_set[url].append(link)\n",
    "            else:\n",
    "                inlinks_set[url]=inlink_list\n",
    "\n",
    "with open(f'{INDEX_OUTPUT_FOLDER}/links/merged_inlinks.json','w') as final_inlinks_file:\n",
    "    json.dump(inlinks_set,final_inlinks_file,indent=2)\n",
    "    print(\"Merging process complete for inlinks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad32a59b-4814-46c4-a2eb-0cc78922dab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging process complete for outlinks\n"
     ]
    }
   ],
   "source": [
    "outlinks_set={}\n",
    "for outlink_file in os.listdir(OUTLINKS_FOLDER):\n",
    "    curr_path=os.path.join(os.getcwd(),f'{OUTLINKS_FOLDER}/{outlink_file}')\n",
    "    with open(curr_path,'r') as curr_file:\n",
    "        outlink_data=json.load(curr_file)\n",
    "        for url,outlink_list in outlink_data.items():\n",
    "            if url in outlinks_set:\n",
    "                for link in outlink_list:\n",
    "                    if outlinks_set[url].count(link) == 0:\n",
    "                        outlinks_set[url].append(link)\n",
    "            else:\n",
    "                outlinks_set[url]=outlink_list\n",
    "\n",
    "with open(f'{INDEX_OUTPUT_FOLDER}/links/merged_outlinks.json','w') as final_outlinks_file:\n",
    "    json.dump(outlinks_set,final_outlinks_file,indent=2)\n",
    "    print(\"Merging process complete for outlinks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe5a1aa-afde-4966-a3e4-d20127a778a1",
   "metadata": {},
   "source": [
    "## Processing Web Content\n",
    "\n",
    "### Pre-Requisites\n",
    " - Have a folder called 'webdata' that has all the files from all the threads that the crawler created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52d2ff11-cd5c-41d9-83a2-f18eab1734fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEBDATA_FOLDER='./webdata'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59169785-b12d-4395-b769-da70ee9c7439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of stopwords: 531\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "\n",
    "swPath = \"./stoplist.txt\"\n",
    "\n",
    "with open(swPath) as file:\n",
    "    stopwords = file.readlines()\n",
    "    for index, stopword in enumerate(stopwords):\n",
    "        stopwords[index] = stopword.split(\"\\n\")[0]\n",
    "        \n",
    "# Adding punctuations in the stopwords list\n",
    "punctuations = list(string.punctuation)\n",
    "\n",
    "# Extra\n",
    "extraPunc = [\"``\", \"'s'\", \"'\", \"''\"]\n",
    "[punctuations.append(el) for el in extraPunc]\n",
    "\n",
    "for p in punctuations:\n",
    "    stopwords.append(p)\n",
    "        \n",
    "print(f'Total number of stopwords: {len(stopwords)}')\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def inStopWords(word):\n",
    "    return word.lower() in stopwords\n",
    "    \n",
    "def processWord(word):\n",
    "    return ps.stem(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7f7cd18-8ebe-44c8-9d8b-cc7ae363ba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def number(word):\n",
    "    if word[0].isdigit() or word.isdigit():\n",
    "        return True\n",
    "    \n",
    "    try:\n",
    "        float_value = float(word)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "    \n",
    "# Helper method for processing text\n",
    "def processText(text):\n",
    "    words = word_tokenize(text)\n",
    "    processedText = []\n",
    "    \n",
    "    for index, word in enumerate(words):\n",
    "        if not number(word):\n",
    "            processedText.append(processWord(word))\n",
    "            \n",
    "    return \" \".join(processedText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5842c8d3-08db-426a-a2bf-0243654045ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 49\n",
      "On index: 0\n",
      "On index: 1\n",
      "On index: 2\n",
      "On index: 3\n",
      "On index: 4\n",
      "On index: 5\n",
      "On index: 6\n",
      "On index: 7\n",
      "On index: 8\n",
      "On index: 9\n",
      "On index: 10\n",
      "On index: 11\n",
      "On index: 12\n",
      "On index: 13\n",
      "On index: 14\n",
      "On index: 15\n",
      "On index: 16\n",
      "On index: 17\n",
      "On index: 18\n",
      "On index: 19\n",
      "On index: 20\n",
      "On index: 21\n",
      "On index: 22\n",
      "On index: 23\n",
      "On index: 24\n",
      "On index: 25\n",
      "On index: 26\n",
      "On index: 27\n",
      "On index: 28\n",
      "On index: 29\n",
      "On index: 30\n",
      "On index: 31\n",
      "On index: 32\n",
      "On index: 33\n",
      "On index: 34\n",
      "On index: 35\n",
      "On index: 36\n",
      "On index: 37\n",
      "On index: 38\n",
      "On index: 39\n",
      "On index: 40\n",
      "On index: 41\n",
      "On index: 42\n",
      "On index: 43\n",
      "On index: 44\n",
      "On index: 45\n",
      "On index: 46\n",
      "On index: 47\n",
      "On index: 48\n",
      "Total empty inlinks : 0\n",
      "Total empty outlinks : 44\n",
      "Total empty content files : 7163\n"
     ]
    }
   ],
   "source": [
    "file_id=1\n",
    "empty_inlinks=0\n",
    "empty_outlinks=0\n",
    "empty_content=0\n",
    "\n",
    "\n",
    "webdata_files = os.listdir(WEBDATA_FOLDER)\n",
    "\n",
    "print(f'Total files: {len(webdata_files)}')\n",
    "\n",
    "for index, webdata_file in enumerate(webdata_files):\n",
    "    print(f'On index: {index}')\n",
    "    \n",
    "    index_data={}\n",
    "    curr_web_file=os.path.join(os.getcwd(),f'{WEBDATA_FOLDER}/{webdata_file}')\n",
    "    with open(curr_web_file,'r',encoding='utf-8') as curr_file:\n",
    "        curr_web_data=curr_file.read()\n",
    "        documents=re.findall(DOC_PATTERN,curr_web_data,re.DOTALL)\n",
    "        for document in documents:\n",
    "            curr_index_info={}\n",
    "            doc_url=re.findall(DOCNO_PATTERN,document,re.DOTALL)[0]\n",
    "            doc_text=re.findall(TEXT_PATTERN,document,re.DOTALL)\n",
    "            doc_text='\\n'.join(doc_text)\n",
    "            head_text=re.findall(HEAD_PATTERN,document,re.DOTALL)\n",
    "            head_text='\\n'.join(head_text)\n",
    "            content_text = processText(doc_text) \n",
    "            if \"not found\" in head_text.lower() or content_text == \"\":\n",
    "                empty_content+=1\n",
    "                continue\n",
    "            \n",
    "            curr_index_info[\"content\"]=content_text\n",
    "            curr_index_info[\"title\"]=head_text\n",
    "            curr_index_info[\"inlinks\"]=inlinks_set.get(doc_url,[])\n",
    "            curr_index_info[\"outlinks\"]=outlinks_set.get(doc_url,[])\n",
    "            if(len(curr_index_info[\"inlinks\"])==0):\n",
    "                empty_inlinks+=1\n",
    "            if(len(curr_index_info[\"outlinks\"])==0):\n",
    "                empty_outlinks+=1\n",
    "            index_data[doc_url]=curr_index_info\n",
    "        with open(f'{INDEX_OUTPUT_FOLDER}/index_file_{file_id}.json','w') as index_file:\n",
    "            json.dump(index_data,index_file,indent=2)\n",
    "        file_id+=1\n",
    "        \n",
    "print(f\"Total empty inlinks : {empty_inlinks}\")\n",
    "print(f\"Total empty outlinks : {empty_outlinks}\")\n",
    "print(f\"Total empty content files : {empty_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8d5093-a055-48a5-a8ff-76ec9bbdaa5e",
   "metadata": {},
   "source": [
    "## Merging all index files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0465dca-d1fb-4335-8ceb-39a7950b04ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with file: ./index_op/index_file_25.json\n",
      "Working with file: ./index_op/index_file_33.json\n",
      "Working with file: ./index_op/index_file_48.json\n",
      "Working with file: ./index_op/index_file_29.json\n",
      "Working with file: ./index_op/index_file_7.json\n",
      "Working with file: ./index_op/index_file_44.json\n",
      "Working with file: ./index_op/index_file_13.json\n",
      "Working with file: ./index_op/index_file_12.json\n",
      "Working with file: ./index_op/index_file_45.json\n",
      "Working with file: ./index_op/index_file_6.json\n",
      "Working with file: ./index_op/index_file_28.json\n",
      "Working with file: ./index_op/index_file_49.json\n",
      "Working with file: ./index_op/index_file_32.json\n",
      "Working with file: ./index_op/index_file_24.json\n",
      "Working with file: ./index_op/index_file_39.json\n",
      "Working with file: ./index_op/index_file_1.json\n",
      "Working with file: ./index_op/index_file_42.json\n",
      "Working with file: ./index_op/index_file_15.json\n",
      "Working with file: ./index_op/index_file_23.json\n",
      "Working with file: ./index_op/index_file_35.json\n",
      "Working with file: ./index_op/index_file_19.json\n",
      "Working with file: ./index_op/index_file_18.json\n",
      "Working with file: ./index_op/index_file_34.json\n",
      "Working with file: ./index_op/index_file_22.json\n",
      "Working with file: ./index_op/index_file_14.json\n",
      "Working with file: ./index_op/index_file_43.json\n",
      "Working with file: ./index_op/index_file_38.json\n",
      "Working with file: ./index_op/index_file_17.json\n",
      "Working with file: ./index_op/index_file_40.json\n",
      "Working with file: ./index_op/index_file_3.json\n",
      "Working with file: ./index_op/index_file_37.json\n",
      "Working with file: ./index_op/index_file_21.json\n",
      "Working with file: ./index_op/index_file_20.json\n",
      "Working with file: ./index_op/index_file_36.json\n",
      "Working with file: ./index_op/index_file_2.json\n",
      "Working with file: ./index_op/index_file_41.json\n",
      "Working with file: ./index_op/index_file_16.json\n",
      "Working with file: ./index_op/index_file_9.json\n",
      "Working with file: ./index_op/index_file_31.json\n",
      "Working with file: ./index_op/index_file_27.json\n",
      "Working with file: ./index_op/index_file_11.json\n",
      "Working with file: ./index_op/index_file_46.json\n",
      "Working with file: ./index_op/index_file_5.json\n",
      "Working with file: ./index_op/index_file_4.json\n",
      "Working with file: ./index_op/index_file_47.json\n",
      "Working with file: ./index_op/index_file_10.json\n",
      "Working with file: ./index_op/index_file_26.json\n",
      "Working with file: ./index_op/index_file_30.json\n",
      "Working with file: ./index_op/index_file_8.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Initialize the merged data structure\n",
    "merged_data = defaultdict(lambda: {\n",
    "    \"content\": \"\",\n",
    "    \"title\": \"\",\n",
    "    \"inlinks\": set(),\n",
    "    \"outlinks\": set(),\n",
    "})\n",
    "\n",
    "for json_file in os.listdir(INDEX_OUTPUT_FOLDER):\n",
    "    # Construct the full path to the current file\n",
    "    curr_path = os.path.join(INDEX_OUTPUT_FOLDER, json_file)\n",
    "\n",
    "    if \"json\" not in curr_path or os.path.isdir(curr_path):\n",
    "            continue\n",
    "        \n",
    "    # Open and read the JSON file\n",
    "    with open(curr_path, 'r') as file:\n",
    "        file_data = json.load(file)\n",
    "        print(f\"Working with file: {curr_path}\")\n",
    "        \n",
    "        # Merge each URL's data\n",
    "        for url, data in file_data.items():        \n",
    "            merged_data[url][\"content\"] = merged_data[url][\"content\"] or data.get(\"content\", \"\")\n",
    "            merged_data[url][\"title\"] = merged_data[url][\"title\"] or data.get(\"title\", \"\")\n",
    "            \n",
    "            # Merge inlinks and outlinks using set\n",
    "            merged_data[url][\"inlinks\"].update(data.get(\"inlinks\", []))\n",
    "            merged_data[url][\"outlinks\"].update(data.get(\"outlinks\", []))\n",
    "\n",
    "# Convert sets back to lists for the final output\n",
    "for url in merged_data:\n",
    "    merged_data[url][\"inlinks\"] = list(merged_data[url][\"inlinks\"])\n",
    "    merged_data[url][\"outlinks\"] = list(merged_data[url][\"outlinks\"])\n",
    "\n",
    "# Save the merged data to a file or use it as needed\n",
    "output_path = \"final_index.json\"\n",
    "with open(output_path, 'w') as outfile:\n",
    "    json.dump(merged_data, outfile, indent=4)\n",
    "\n",
    "print(f\"Merged JSON saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01abbe6-09ec-4fa4-bf8a-c0b9f6d3ce00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
